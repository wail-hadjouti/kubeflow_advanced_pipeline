name: Merge and split
metadata:
  annotations: {author: Antoine Villatte}
outputs:
- {name: output_edfcsv, type: CSV}
implementation:
  container:
    image: public.ecr.aws/f6t4n1w1/poc_kf_pipeline:latest
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n   \
      \ os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
      \ndef merge_and_split(output_edfcsv):\n\n    import pandas as pd\n    import\
      \ re\n    from io import StringIO\n    from sklearn.model_selection import GroupShuffleSplit\n\
      \n    def get_location(text, method):\n        \"\"\"Retrieves data from the\
      \ 2015 dataset to harmonize it with the 2016 dataset\"\"\"\n        if method\
      \ == \"latitude\":\n            match = re.search('{\\'latitude\\': \\'(.+?)\\\
      ', \\'longitude', text)\n        elif method == \"longitude\":\n           \
      \ match = re.search('[0-9]\\', \\'longitude\\': \\'(.+?)\\', \\'human_address\\\
      '', text)\n        elif method == \"address\":\n            match = re.search('\\\
      '{\"address\": \"(.+?)\", \"city\":', text)      \n        elif method == \"\
      city\":\n            match = re.search('\", \"city\": \"(.+?)\", \"state\":',\
      \ text)\n        elif method == \"state\":\n            match = re.search('\"\
      state\": \"(.+?)\", \"zip\":', text)\n        elif method == \"zip\":\n    \
      \        match = re.search('\"zip\": \"(.+?)\"}\\'}', text)\n        else:\n\
      \            raise ValueError(\"Veuillez choisir une m\xE9thode (latitude, longitude,\
      \ adress, city, state, zip)\")\n\n        if match:\n            found = match.group(1)\n\
      \            return(found)\n        return(\"N/A\")\n\n    # Load datasets\n\
      \    #csv_strings = {}\n    #encoding = 'utf-8'\n    #for source_data, year\
      \ in zip([data_2015, data_2016], [\"2015\", \"2016\"]):\n        #csv_obj =\
      \ boto3.client('s3').get_object(Bucket=bucket, Key=source_data)\n        #body\
      \ = csv_obj['Body']\n        #csv_string = body.read().decode(encoding)\n  \
      \      #csv_strings[year] = csv_string\n    url2015 = \"https://raw.githubusercontent.com/wail-hadjouti/kubeflow_advanced_pipeline/master/data/2015-building-energy-benchmarking.csv\"\
      \n    url2016 = \"https://raw.githubusercontent.com/wail-hadjouti/kubeflow_advanced_pipeline/master/data/2015-building-energy-benchmarking.csv\"\
      \n\n    data15 = pd.read_csv(url2015)\n    data16 = pd.read_csv(url2016)\n\n\
      \    # Rename mismatched columns\n    rename_cols = {\n        \"GHGEmissions(MetricTonsCO2e)\"\
      \ : \"TotalGHGEmissions\",\n        \"GHGEmissionsIntensity(kgCO2e/ft2)\" :\
      \ \"GHGEmissionsIntensity\",\n        \"Comment\" : \"Comments\"\n    }\n  \
      \  data15.rename(columns = rename_cols, inplace = True)\n\n    # Extract location\
      \ info from 2015 dataset to harmonize it\n    data15[\"Latitude\"] = data15[\"\
      Location\"].apply(get_location, method = \"latitude\")\n    data15[\"Longitude\"\
      ] = data15[\"Location\"].apply(get_location, method = \"longitude\")\n    data15[\"\
      Address\"] = data15[\"Location\"].apply(get_location, method = \"address\")\n\
      \    data15[\"City\"] = data15[\"Location\"].apply(get_location, method = \"\
      city\")\n    data15[\"State\"] = data15[\"Location\"].apply(get_location, method\
      \ = \"state\")\n    data15[\"ZipCode\"] = data15[\"Location\"].apply(get_location,\
      \ method = \"zip\")\n    data15[\"ZipCode\"] = data15[\"ZipCode\"].astype(int)\
      \ # convert to numeric\n    data15.drop(columns = \"Location\", inplace = True)\n\
      \n    # Delete columns from data15 that arent in data16\n    data15.drop(columns\
      \ = set(data15.columns.tolist()).difference(data16.columns.tolist()), inplace\
      \ = True)\n\n    # Harmonize column order\n    cols_order = data16.columns.tolist()\n\
      \    data15 = data15.loc[:, cols_order]\n\n    # Concatenate\n    emission_df\
      \ = pd.concat([data15, data16], axis = 0, ignore_index = True)\n\n    # Train/test\
      \ split\n    inTrain , inTest = next(GroupShuffleSplit(train_size = 0.7, random_state\
      \ = 42).\\\n                        split(emission_df, groups = emission_df[\"\
      OSEBuildingID\"]))\n\n    emission_df[\"in_train\"] = 0\n    emission_df.iloc[inTrain,\
      \ 46] = 1\n\n    emission_df.to_csv(output_edfcsv, index=False)\n\nimport argparse\n\
      _parser = argparse.ArgumentParser(prog='Merge and split', description='')\n\
      _parser.add_argument(\"--output-edfcsv\", dest=\"output_edfcsv\", type=_make_parent_dirs_and_return_path,\
      \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
      \n_outputs = merge_and_split(**_parsed_args)\n"
    args:
    - --output-edfcsv
    - {outputPath: output_edfcsv}
